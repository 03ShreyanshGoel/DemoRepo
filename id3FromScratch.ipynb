{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8gPtiZE2TrbnB6Liwn8jc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/03ShreyanshGoel/DemoRepo/blob/main/id3FromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def entropy(data, target_attr):\n",
        "    \"\"\"Calculates the entropy of a dataset for a given target attribute.\"\"\"\n",
        "    val_freq = {}\n",
        "    data_entropy = 0.0\n",
        "\n",
        "    # Calculate the frequency of each of the values in the target attribute\n",
        "    for record in data:\n",
        "        if record[target_attr] in val_freq:\n",
        "            val_freq[record[target_attr]] += 1.0\n",
        "        else:\n",
        "            val_freq[record[target_attr]] = 1.0\n",
        "\n",
        "    # Calculate the entropy of the data for the target attribute\n",
        "    for freq in val_freq.values():\n",
        "        data_entropy += (-freq / len(data)) * math.log(freq / len(data), 2)\n",
        "\n",
        "    return data_entropy\n",
        "\n",
        "def gain(data, attr, target_attr):\n",
        "    \"\"\"Calculates the information gain (reduction in entropy) that would\n",
        "    result by splitting the data on the chosen attribute (attr).\"\"\"\n",
        "    val_freq = {}\n",
        "    subset_entropy = 0.0\n",
        "\n",
        "    # Calculate the frequency of each of the values in the target attribute\n",
        "    for record in data:\n",
        "        if record[attr] in val_freq:\n",
        "            val_freq[record[attr]] += 1.0\n",
        "        else:\n",
        "            val_freq[record[attr]] = 1.0\n",
        "\n",
        "    # Calculate the sum of the entropy for each subset of records weighted\n",
        "    # by their probability of occuring in the training set.\n",
        "    for val in val_freq.keys():\n",
        "        val_prob = val_freq[val] / sum(val_freq.values())\n",
        "        data_subset = [record for record in data if record[attr] == val]\n",
        "        subset_entropy += val_prob * entropy(data_subset, target_attr)\n",
        "\n",
        "    # Subtract the entropy of the chosen attribute from the entropy of the\n",
        "    # whole data set with respect to the target attribute (and return it)\n",
        "    return entropy(data, target_attr) - subset_entropy\n",
        "\n",
        "def majority_value(data, target_attr):\n",
        "    \"\"\"Creates a list of all values in the target attribute for each record\n",
        "    in the data list object, and returns the value that appears in this list\n",
        "    the most frequently.\"\"\"\n",
        "    val_freq = {}\n",
        "    # Calculate the frequency of each of the values in the target attribute\n",
        "    for record in data:\n",
        "        if record[target_attr] in val_freq:\n",
        "            val_freq[record[target_attr]] += 1\n",
        "        else:\n",
        "            val_freq[record[target_attr]] = 1\n",
        "    max_freq = 0\n",
        "    major_val = \"\"\n",
        "    for val, freq in val_freq.items():\n",
        "        if freq > max_freq:\n",
        "            max_freq = freq\n",
        "            major_val = val\n",
        "    return major_val\n",
        "\n",
        "def get_values(data, attr):\n",
        "    \"\"\"Creates a list of values in the given attribute for each record in data,\n",
        "    prunes out all of the redundant values, and returns the list.\"\"\"\n",
        "    return list(set([record[attr] for record in data]))\n",
        "\n",
        "def choose_attribute(data, attributes, target_attr):\n",
        "    \"\"\"Cycles through all the attributes and returns the attribute with the\n",
        "    highest information gain (or the one with the most entropy reduction).\"\"\"\n",
        "    best_gain = 0.0\n",
        "    best_attr = None\n",
        "\n",
        "    for attr in attributes:\n",
        "        gain_val = gain(data, attr, target_attr)\n",
        "        if gain_val > best_gain and attr != target_attr:\n",
        "            best_gain = gain_val\n",
        "            best_attr = attr\n",
        "\n",
        "    return best_attr\n",
        "\n",
        "def create_decision_tree(data, attributes, target_attr, recursion_depth=0, max_depth=10):\n",
        "    \"\"\"Returns a new decision tree based on the examples given.\"\"\"\n",
        "    data = data[:]\n",
        "    vals = [record[target_attr] for record in data]\n",
        "\n",
        "    default = majority_value(data, target_attr)\n",
        "\n",
        "    # If the dataset is empty or the attributes list is empty, return the\n",
        "    # default value. When checking the attributes list for emptiness, we\n",
        "    # need to subtract 1 to account for the target attribute.\n",
        "    if not data or (len(attributes) - 1) <= 0 or recursion_depth >= max_depth:\n",
        "        return default\n",
        "\n",
        "    # If all the records in the dataset have the same classification,\n",
        "    # return that classification.\n",
        "    elif vals.count(vals[0]) == len(vals):\n",
        "        return vals[0]\n",
        "\n",
        "    else:\n",
        "        # Choose the next best attribute to best classify our data\n",
        "        best = choose_attribute(data, attributes, target_attr)\n",
        "\n",
        "        # Create a new decision tree/node with the best attribute and an empty\n",
        "        # dictionary object--we'll fill that up next.\n",
        "        tree = {best: {}}\n",
        "\n",
        "        # Create a new decision tree/sub-node for each of the values in the\n",
        "        # best attribute field\n",
        "        for val in get_values(data, best):\n",
        "            # Create a subtree for the current value under the \"best\" field\n",
        "            subtree = create_decision_tree(\n",
        "                [record for record in data if record[best] == val],\n",
        "                [attr for attr in attributes if attr != best],\n",
        "                target_attr,\n",
        "                recursion_depth + 1,\n",
        "                max_depth\n",
        "            )\n",
        "\n",
        "            # Add the new subtree to the empty dictionary object in our new\n",
        "            # tree/node we just created.\n",
        "            tree[best][val] = subtree\n",
        "\n",
        "    return tree"
      ],
      "metadata": {
        "id": "WYHgZ1lKwkcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play': False},\n",
        "    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': True, 'play': False},\n",
        "    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': False, 'play': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': False, 'play': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': True, 'play': False},\n",
        "    {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': True, 'play': True},\n",
        "    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': False, 'play': False},\n",
        "    {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': False, 'play': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'normal', 'windy': False, 'play': True},\n",
        "    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': True, 'play': True},\n",
        "    {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': True, 'play': True},\n",
        "    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': False, 'play': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': True, 'play': False}\n",
        "]\n",
        "\n",
        "# Attributes\n",
        "attributes = ['outlook', 'temperature', 'humidity', 'windy']\n",
        "\n",
        "# Target attribute\n",
        "target_attr = 'play'\n",
        "\n",
        "# Create the decision tree\n",
        "tree = create_decision_tree(data, attributes, target_attr)\n",
        "\n",
        "# Print the decision tree\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq5tZX7OwsWx",
        "outputId": "af220740-500f-4d5d-92ff-ed33977837aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'outlook': {'sunny': {'humidity': {'normal': True, 'high': False}}, 'rainy': {'windy': {False: True, True: False}}, 'overcast': True}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "   # Assuming 'data' is your dataset\n",
        "   # Assuming 'attributes' is the list of attribute names\n",
        "   # Assuming 'target_attr' is the name of the target attribute\n",
        "\n",
        "   # Create feature data (X) and target data (y)\n",
        "X = [[record[attr] for attr in attributes] for record in data]\n",
        "y = [record[target_attr] for record in data]\n",
        "\n",
        "   # Split data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "yQcdXiibyAar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the decision tree using your ID3 implementation\n",
        "tree_dict = create_decision_tree(data, attributes, target_attr) #data should be replaced by X_train"
      ],
      "metadata": {
        "id": "IZZ19F2synvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tree, instance):\n",
        "       \"\"\"Predicts the class label for a given instance using the decision tree.\"\"\"\n",
        "       if not isinstance(tree, dict):  # If it's a leaf node (not a dictionary)\n",
        "           return tree\n",
        "\n",
        "       attribute = list(tree.keys())[0]  # Get the attribute at the current node\n",
        "\n",
        "       if instance[attributes.index(attribute)] in tree[attribute]:\n",
        "           # Recursively traverse the tree\n",
        "           return predict(tree[attribute][instance[attributes.index(attribute)]], instance)\n",
        "       else:\n",
        "           # Handle cases where the attribute value is not in the tree (use majority class)\n",
        "           # or implement other handling strategies as needed.\n",
        "           # for now , will return False\n",
        "           return False"
      ],
      "metadata": {
        "id": "adEpLzYeyvip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "predictions = [predict(tree_dict, instance) for instance in X_test]"
      ],
      "metadata": {
        "id": "4eFxRfNYyzBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "   # Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BWugOTSy9BM",
        "outputId": "66289a8f-d4ef-4224-b130-f90bd57e9b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}